{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What mutation rate should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have simulated noisy data with average of 1, 2, 3, and 4 mutations. If we fit a model to each of them, how good are the fits? \n",
    "Each library has 30,000 variants and we will use the same parameters in each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>library</th>\n",
       "      <th>aa_substitutions</th>\n",
       "      <th>concentration</th>\n",
       "      <th>prob_escape</th>\n",
       "      <th>IC90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td></td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.087480</td>\n",
       "      <td>0.1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td></td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.034240</td>\n",
       "      <td>0.1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td></td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.037880</td>\n",
       "      <td>0.1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td></td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.035730</td>\n",
       "      <td>0.1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td></td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359995</th>\n",
       "      <td>avg2muts</td>\n",
       "      <td>Y473E L518F D427L</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>1.1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359996</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td>Y473S G413Q</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359997</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td>Y473V P479R F392W</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>1.4550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359998</th>\n",
       "      <td>avg3muts</td>\n",
       "      <td>Y489Q N501Y</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359999</th>\n",
       "      <td>avg2muts</td>\n",
       "      <td>Y505N H519T</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         library   aa_substitutions  concentration  prob_escape    IC90\n",
       "0       avg1muts                              0.25     0.087480  0.1128\n",
       "1       avg1muts                              0.25     0.034240  0.1128\n",
       "2       avg1muts                              0.25     0.037880  0.1128\n",
       "3       avg1muts                              0.25     0.035730  0.1128\n",
       "4       avg1muts                              0.25     0.000000  0.1128\n",
       "...          ...                ...            ...          ...     ...\n",
       "359995  avg2muts  Y473E L518F D427L           4.00     0.002918  1.1600\n",
       "359996  avg1muts        Y473S G413Q           4.00     0.000000  0.5780\n",
       "359997  avg1muts  Y473V P479R F392W           4.00     0.160200  1.4550\n",
       "359998  avg3muts        Y489Q N501Y           4.00     0.000000  0.5881\n",
       "359999  avg2muts        Y505N H519T           4.00     0.000000  0.3505\n",
       "\n",
       "[360000 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polyclonal\n",
    "\n",
    "noisy_data = (\n",
    "    pd.read_csv('RBD_variants_escape_noisy.csv', na_filter=None)\n",
    "    .query('concentration in [0.25, 1, 4]')\n",
    "    .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "noisy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the dataset for each library into separate output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset written to torchdms_results/noisy_3conc_1muts.\n",
      "Dataset written to torchdms_results/noisy_3conc_2muts.\n",
      "Dataset written to torchdms_results/noisy_3conc_3muts.\n",
      "Dataset written to torchdms_results/noisy_3conc_4muts.\n"
     ]
    }
   ],
   "source": [
    "import torchdms\n",
    "import Bio.SeqIO\n",
    "import pickle\n",
    "\n",
    "wtseq_dna = Bio.SeqIO.read('RBD_seq.fasta', 'fasta').seq\n",
    "wtseq_aa = str(wtseq_dna.translate())\n",
    "assert len(wtseq_aa) == 201\n",
    "\n",
    "def recode_aa_subs(old_aa_subs, offset):\n",
    "    \"\"\"\n",
    "    recode amino acid substitutions by a specific offset\n",
    "    ex. A331B -> A1B by a 330bp offset.\n",
    "    \"\"\"\n",
    "    subs = old_aa_subs.split()\n",
    "    recoded_subs = []\n",
    "    for s in subs:\n",
    "        wt = s[0]\n",
    "        mut = s[-1]\n",
    "        pos = int(s[1:-1]) - offset\n",
    "        recoded_subs.append(wt + str(pos) + mut)\n",
    "    recoded_subs = ' '.join(recoded_subs)\n",
    "    return recoded_subs\n",
    "\n",
    "def format_data_for_torchdms(df):\n",
    "    \"\"\"\n",
    "    Format the simulated data to be compatible\n",
    "    with torchdms input.\n",
    "    1. remove any NaN\n",
    "    2. 1-index all aa_substitutions\n",
    "    \"\"\"\n",
    "    data = df.fillna('')\n",
    "    data['aa_substitutions'] = data['aa_substitutions'].apply(\n",
    "        lambda x: recode_aa_subs(x, 330)\n",
    "    )\n",
    "    return data\n",
    "\n",
    "for n in [1,2,3,4]:\n",
    "    avg_n_data = noisy_data.query(f\"library == 'avg{n}muts'\")\n",
    "    formatted_data = format_data_for_torchdms(avg_n_data)\n",
    "    assert len(formatted_data.index == 90000)\n",
    "    with open(f\"torchdms_results/noisy_3conc_{n}muts/noisy_3conc_{n}muts_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump([formatted_data, wtseq_aa], f)\n",
    "    print(f\"Dataset written to torchdms_results/noisy_3conc_{n}muts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd /fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the `torchdms` models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks/torchdms_results/noisy_3conc_1muts\n",
      "Prepping dataset.\n",
      "LOG: Targets: ('prob_escape',)\n",
      "LOG: There are 90000 total variants in this dataset\n",
      "LOG: Partitioning data via 'avg1muts'\n",
      "LOG: There are 26409 training samples for stratum: 1\n",
      "LOG: There are 15612 training samples for stratum: 2\n",
      "LOG: There are 4417 training samples for stratum: 3\n",
      "LOG: There are 297 training samples for stratum: 4\n",
      "LOG: There are 4793 validation samples\n",
      "LOG: There are 4898 test samples\n",
      "LOG: Successfully partitioned data\n",
      "LOG: preparing binary map dataset\n",
      "LOG: Successfully finished prep and dumped SplitDataset object to prepped\n",
      "Training model.\n",
      "LOG: Setting random seed to 0.\n",
      "LOG: Model defined as: Escape(\n",
      "  (latent_layer_epi0): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi1): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi2): Linear(in_features=4221, out_features=1, bias=False)\n",
      ")\n",
      "{'num_epitopes': 3, 'beta_l1_coefficient': 0.0}\n",
      "LOG: Saved model to run.model\n",
      "LOG: Setting random seed to 0.\n",
      "Exponentiating targets with base 1.\n",
      "Starting training. {'independent_start_count': 0, 'independent_start_epoch_count': None, 'epoch_count': 50, 'loss_fn': <function weighted_loss.<locals>.wrapper at 0x14ddcca318b0>, 'patience': 10, 'min_lr': 1e-05, 'loss_weight_span': None, 'exp_target': 1, 'beta_rank': None}\n",
      "LOG: Beginning full training.\n",
      "Training in default style.\n",
      "\n",
      "validation loss record: 0.10856586694717407\n",
      "\u001b[?25l  [------------------------------------]    2%  00:01:23\n",
      "validation loss record: 0.10465539991855621\n",
      "\u001b[?25l  [#-----------------------------------]    4%  00:01:21\n",
      "validation loss record: 0.101560078561306\n",
      "\u001b[?25l  [##----------------------------------]    6%  00:01:20\n",
      "validation loss record: 0.09923920035362244\n",
      "\u001b[?25l  [##----------------------------------]    8%  00:01:19\n",
      "validation loss record: 0.09724277257919312\n",
      "\u001b[?25l  [###---------------------------------]   10%  00:01:17\n",
      "validation loss record: 0.095624640583992\n",
      "\u001b[?25l  [####--------------------------------]   12%  00:01:16\n",
      "validation loss record: 0.09441199898719788\n",
      "\u001b[?25l  [#####-------------------------------]   14%  00:01:14\n",
      "validation loss record: 0.09349720925092697\n",
      "\u001b[?25l  [#####-------------------------------]   16%  00:01:13\n",
      "validation loss record: 0.09268751740455627\n",
      "\u001b[?25l  [######------------------------------]   18%  00:01:11\n",
      "validation loss record: 0.09207890182733536\n",
      "\u001b[?25l  [#######-----------------------------]   20%  00:01:09\n",
      "validation loss record: 0.0916246548295021\n",
      "\u001b[?25l  [#######-----------------------------]   22%  00:01:07\n",
      "validation loss record: 0.09114759415388107\n",
      "\u001b[?25l  [########----------------------------]   24%  00:01:05\n",
      "validation loss record: 0.09081669896841049\n",
      "\u001b[?25l  [#########---------------------------]   26%  00:01:04\n",
      "validation loss record: 0.09057028591632843\n",
      "\u001b[?25l  [##########--------------------------]   28%  00:01:02\n",
      "validation loss record: 0.09034223109483719\n",
      "\u001b[?25l  [##########--------------------------]   30%  00:01:00\n",
      "validation loss record: 0.09022416919469833\n",
      "\u001b[?25l  [###########-------------------------]   32%  00:00:59\n",
      "validation loss record: 0.09004863351583481\n",
      "\u001b[?25l  [############------------------------]   34%  00:00:57\n",
      "validation loss record: 0.08996223658323288\n",
      "\u001b[?25l  [############------------------------]   36%  00:00:55\n",
      "validation loss record: 0.08980677276849747\n",
      "\u001b[?25l  [#############-----------------------]   38%  00:00:53\n",
      "validation loss record: 0.08969047665596008\n",
      "\u001b[?25l  [##############----------------------]   40%  00:00:51\n",
      "validation loss record: 0.08955316990613937\n",
      "\u001b[?25l  [###############---------------------]   42%  00:00:49\n",
      "validation loss record: 0.08947731554508209\n",
      "\u001b[?25l  [###############---------------------]   44%  00:00:47\n",
      "validation loss record: 0.08946538716554642\n",
      "\u001b[?25l  [################--------------------]   46%  00:00:45\n",
      "validation loss record: 0.08942800015211105\n",
      "\u001b[?25l  [#################-------------------]   48%  00:00:43\n",
      "validation loss record: 0.08942151814699173\n",
      "\u001b[?25l  [##################------------------]   50%  00:00:42\n",
      "validation loss record: 0.08938625454902649\n",
      "\u001b[?25l  [##################------------------]   52%  00:00:40\n",
      "validation loss record: 0.08935625106096268\n",
      "\u001b[?25l  [##########################----------]   74%  00:00:21Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "\u001b[?25l  [##################################--]   96%  00:00:03Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[?25l  [####################################]  100%          \u001b[?25h\n",
      "LOG: error plot finished and dumped to run.error.pdf\n",
      "prob_escape: Escape\n",
      "pearsonr = 0.7\n",
      "LOG: scatter plot finished and dumped to run.scatter.pdf\n",
      "WARNING: I don't know how to make a GE plot for any other dims other than 1 and 2.\n",
      "LOG: loaded data, evaluating beta coeff for wildtype seq: NITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKST\n",
      "LOG: Beta coefficients plotted and dumped to run.beta.pdf\n",
      "LOG: model loaded, calculating SVD for beta coefficents.\n",
      "LOG: Singular values of beta plotted and dumped to run.svd.pdf\n",
      "LOG: model loaded, plotting amino acid and site profiles.\n",
      "LOG: Amino acid and site profiles plotted and dumped to run.profiles.pdf\n",
      "LOG: `tdms go` completed; touching run.sentinel\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks/torchdms_results/noisy_3conc_2muts\n",
      "Prepping dataset.\n",
      "LOG: Targets: ('prob_escape',)\n",
      "LOG: There are 90000 total variants in this dataset\n",
      "LOG: Partitioning data via 'avg2muts'\n",
      "LOG: There are 17965 training samples for stratum: 1\n",
      "LOG: There are 22493 training samples for stratum: 2\n",
      "LOG: There are 15017 training samples for stratum: 3\n",
      "LOG: There are 6833 training samples for stratum: 4\n",
      "LOG: There are 1845 training samples for stratum: 5\n",
      "LOG: There are 6047 validation samples\n",
      "LOG: There are 5994 test samples\n",
      "LOG: Successfully partitioned data\n",
      "LOG: preparing binary map dataset\n",
      "LOG: Successfully finished prep and dumped SplitDataset object to prepped\n",
      "Training model.\n",
      "LOG: Setting random seed to 0.\n",
      "LOG: Model defined as: Escape(\n",
      "  (latent_layer_epi0): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi1): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi2): Linear(in_features=4221, out_features=1, bias=False)\n",
      ")\n",
      "{'num_epitopes': 3, 'beta_l1_coefficient': 0.0}\n",
      "LOG: Saved model to run.model\n",
      "LOG: Setting random seed to 0.\n",
      "Exponentiating targets with base 1.\n",
      "Starting training. {'independent_start_count': 0, 'independent_start_epoch_count': None, 'epoch_count': 50, 'loss_fn': <function weighted_loss.<locals>.wrapper at 0x14d60aef18b0>, 'patience': 10, 'min_lr': 1e-05, 'loss_weight_span': None, 'exp_target': 1, 'beta_rank': None}\n",
      "LOG: Beginning full training.\n",
      "Training in default style.\n",
      "\n",
      "validation loss record: 0.14168386161327362\n",
      "\u001b[?25l  [------------------------------------]    2%  00:01:18\n",
      "validation loss record: 0.12604644894599915\n",
      "\u001b[?25l  [#-----------------------------------]    4%  00:01:16\n",
      "validation loss record: 0.118702232837677\n",
      "\u001b[?25l  [##----------------------------------]    6%  00:01:13\n",
      "validation loss record: 0.11471076309680939\n",
      "\u001b[?25l  [##----------------------------------]    8%  00:01:12\n",
      "validation loss record: 0.1124366968870163\n",
      "\u001b[?25l  [###---------------------------------]   10%  00:01:11\n",
      "validation loss record: 0.11105069518089294\n",
      "\u001b[?25l  [####--------------------------------]   12%  00:01:10\n",
      "validation loss record: 0.11023614555597305\n",
      "\u001b[?25l  [#####-------------------------------]   14%  00:01:09\n",
      "validation loss record: 0.10993535071611404\n",
      "\u001b[?25l  [#####-------------------------------]   16%  00:01:08\n",
      "validation loss record: 0.10961686074733734\n",
      "\u001b[?25l  [######------------------------------]   18%  00:01:07\n",
      "validation loss record: 0.10951132327318192\n",
      "\u001b[?25l  [#######-----------------------------]   20%  00:01:06\n",
      "validation loss record: 0.109469473361969\n",
      "\u001b[?25l  [#######-----------------------------]   22%  00:01:05\n",
      "validation loss record: 0.10937580466270447\n",
      "\u001b[?25l  [########----------------------------]   24%  00:01:04\n",
      "validation loss record: 0.10925375670194626\n",
      "\u001b[?25l  [#########---------------------------]   26%  00:01:03\n",
      "validation loss record: 0.10914696753025055\n",
      "\u001b[?25l  [##########--------------------------]   30%  00:01:00\n",
      "validation loss record: 0.10907327383756638\n",
      "\u001b[?25l  [###########-------------------------]   32%  00:00:59\n",
      "validation loss record: 0.10902221500873566\n",
      "\u001b[?25l  [############------------------------]   34%  00:00:58\n",
      "validation loss record: 0.1089644655585289\n",
      "\u001b[?25l  [############------------------------]   36%  00:00:56\n",
      "validation loss record: 0.1089194193482399\n",
      "\u001b[?25l  [#############-----------------------]   38%  00:00:54\n",
      "validation loss record: 0.10890746116638184\n",
      "\u001b[?25l  [##############----------------------]   40%  00:00:52\n",
      "validation loss record: 0.10888268798589706\n",
      "\u001b[?25l  [################--------------------]   46%  00:00:46\n",
      "validation loss record: 0.10883136838674545\n",
      "\u001b[?25l  [#################-------------------]   48%  00:00:44\n",
      "validation loss record: 0.10874871164560318\n",
      "\u001b[?25l  [#########################-----------]   70%  00:00:25Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "\u001b[?25l  [#################################---]   92%  00:00:06Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[?25l  [####################################]  100%          \u001b[?25h\n",
      "LOG: error plot finished and dumped to run.error.pdf\n",
      "prob_escape: Escape\n",
      "pearsonr = 0.759\n",
      "LOG: scatter plot finished and dumped to run.scatter.pdf\n",
      "WARNING: I don't know how to make a GE plot for any other dims other than 1 and 2.\n",
      "LOG: loaded data, evaluating beta coeff for wildtype seq: NITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKST\n",
      "LOG: Beta coefficients plotted and dumped to run.beta.pdf\n",
      "LOG: model loaded, calculating SVD for beta coefficents.\n",
      "LOG: Singular values of beta plotted and dumped to run.svd.pdf\n",
      "LOG: model loaded, plotting amino acid and site profiles.\n",
      "LOG: Amino acid and site profiles plotted and dumped to run.profiles.pdf\n",
      "LOG: `tdms go` completed; touching run.sentinel\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks/torchdms_results/noisy_3conc_3muts\n",
      "Prepping dataset.\n",
      "LOG: Targets: ('prob_escape',)\n",
      "LOG: There are 90000 total variants in this dataset\n",
      "LOG: Partitioning data via 'avg3muts'\n",
      "LOG: There are 8377 training samples for stratum: 1\n",
      "LOG: There are 18413 training samples for stratum: 2\n",
      "LOG: There are 17948 training samples for stratum: 3\n",
      "LOG: There are 13620 training samples for stratum: 4\n",
      "LOG: There are 7541 training samples for stratum: 5\n",
      "LOG: There are 2886 training samples for stratum: 6\n",
      "LOG: There are 284 training samples for stratum: 7\n",
      "LOG: There are 7558 validation samples\n",
      "LOG: There are 7565 test samples\n",
      "LOG: Successfully partitioned data\n",
      "LOG: preparing binary map dataset\n",
      "LOG: Successfully finished prep and dumped SplitDataset object to prepped\n",
      "Training model.\n",
      "LOG: Setting random seed to 0.\n",
      "LOG: Model defined as: Escape(\n",
      "  (latent_layer_epi0): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi1): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi2): Linear(in_features=4221, out_features=1, bias=False)\n",
      ")\n",
      "{'num_epitopes': 3, 'beta_l1_coefficient': 0.0}\n",
      "LOG: Saved model to run.model\n",
      "LOG: Setting random seed to 0.\n",
      "Exponentiating targets with base 1.\n",
      "Starting training. {'independent_start_count': 0, 'independent_start_epoch_count': None, 'epoch_count': 50, 'loss_fn': <function weighted_loss.<locals>.wrapper at 0x154e5cae08b0>, 'patience': 10, 'min_lr': 1e-05, 'loss_weight_span': None, 'exp_target': 1, 'beta_rank': None}\n",
      "LOG: Beginning full training.\n",
      "Training in default style.\n",
      "\n",
      "validation loss record: 0.21615155041217804\n",
      "\u001b[?25l  [------------------------------------]    2%  00:01:29\n",
      "validation loss record: 0.18882499635219574\n",
      "\u001b[?25l  [#-----------------------------------]    4%  00:01:26\n",
      "validation loss record: 0.16904862225055695\n",
      "\u001b[?25l  [##----------------------------------]    6%  00:01:24\n",
      "validation loss record: 0.156102254986763\n",
      "\u001b[?25l  [##----------------------------------]    8%  00:01:22\n",
      "validation loss record: 0.14765268564224243\n",
      "\u001b[?25l  [###---------------------------------]   10%  00:01:20\n",
      "validation loss record: 0.14236056804656982\n",
      "\u001b[?25l  [####--------------------------------]   12%  00:01:18\n",
      "validation loss record: 0.13904331624507904\n",
      "\u001b[?25l  [#####-------------------------------]   14%  00:01:17\n",
      "validation loss record: 0.1367000937461853\n",
      "\u001b[?25l  [#####-------------------------------]   16%  00:01:15\n",
      "validation loss record: 0.13564372062683105\n",
      "\u001b[?25l  [######------------------------------]   18%  00:01:13\n",
      "validation loss record: 0.13498179614543915\n",
      "\u001b[?25l  [#######-----------------------------]   20%  00:01:11\n",
      "validation loss record: 0.1346164345741272\n",
      "\u001b[?25l  [#######-----------------------------]   22%  00:01:10\n",
      "validation loss record: 0.1343921422958374\n",
      "\u001b[?25l  [########----------------------------]   24%  00:01:08\n",
      "validation loss record: 0.13426122069358826\n",
      "\u001b[?25l  [############------------------------]   36%  00:00:57\n",
      "validation loss record: 0.13423453271389008\n",
      "\u001b[?25l  [#############-----------------------]   38%  00:00:55\n",
      "validation loss record: 0.1342228502035141\n",
      "\u001b[?25l  [##############----------------------]   40%  00:00:54\n",
      "validation loss record: 0.13419292867183685\n",
      "\u001b[?25l  [######################--------------]   62%  00:00:34Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "\u001b[?25l  [##############################------]   84%  00:00:14Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[?25l  [####################################]  100%          \u001b[?25h\n",
      "LOG: error plot finished and dumped to run.error.pdf\n",
      "prob_escape: Escape\n",
      "pearsonr = 0.841\n",
      "LOG: scatter plot finished and dumped to run.scatter.pdf\n",
      "WARNING: I don't know how to make a GE plot for any other dims other than 1 and 2.\n",
      "LOG: loaded data, evaluating beta coeff for wildtype seq: NITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKST\n",
      "LOG: Beta coefficients plotted and dumped to run.beta.pdf\n",
      "LOG: model loaded, calculating SVD for beta coefficents.\n",
      "LOG: Singular values of beta plotted and dumped to run.svd.pdf\n",
      "LOG: model loaded, plotting amino acid and site profiles.\n",
      "LOG: Amino acid and site profiles plotted and dumped to run.profiles.pdf\n",
      "LOG: `tdms go` completed; touching run.sentinel\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks/torchdms_results/noisy_3conc_4muts\n",
      "Prepping dataset.\n",
      "LOG: Targets: ('prob_escape',)\n",
      "LOG: There are 90000 total variants in this dataset\n",
      "LOG: Partitioning data via 'avg4muts'\n",
      "LOG: There are 3101 training samples for stratum: 1\n",
      "LOG: There are 11116 training samples for stratum: 2\n",
      "LOG: There are 15510 training samples for stratum: 3\n",
      "LOG: There are 15102 training samples for stratum: 4\n",
      "LOG: There are 12307 training samples for stratum: 5\n",
      "LOG: There are 7477 training samples for stratum: 6\n",
      "LOG: There are 3513 training samples for stratum: 7\n",
      "LOG: There are 622 training samples for stratum: 8\n",
      "LOG: There are 8835 validation samples\n",
      "LOG: There are 8774 test samples\n",
      "LOG: Successfully partitioned data\n",
      "LOG: preparing binary map dataset\n",
      "LOG: Successfully finished prep and dumped SplitDataset object to prepped\n",
      "Training model.\n",
      "LOG: Setting random seed to 0.\n",
      "LOG: Model defined as: Escape(\n",
      "  (latent_layer_epi0): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi1): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi2): Linear(in_features=4221, out_features=1, bias=False)\n",
      ")\n",
      "{'num_epitopes': 3, 'beta_l1_coefficient': 0.0}\n",
      "LOG: Saved model to run.model\n",
      "LOG: Setting random seed to 0.\n",
      "Exponentiating targets with base 1.\n",
      "Starting training. {'independent_start_count': 0, 'independent_start_epoch_count': None, 'epoch_count': 50, 'loss_fn': <function weighted_loss.<locals>.wrapper at 0x151f720178b0>, 'patience': 10, 'min_lr': 1e-05, 'loss_weight_span': None, 'exp_target': 1, 'beta_rank': None}\n",
      "LOG: Beginning full training.\n",
      "Training in default style.\n",
      "\n",
      "validation loss record: 0.22014404833316803\n",
      "\u001b[?25l  [------------------------------------]    2%  00:01:25\n",
      "validation loss record: 0.19947601854801178\n",
      "\u001b[?25l  [#-----------------------------------]    4%  00:01:23\n",
      "validation loss record: 0.1809616982936859\n",
      "\u001b[?25l  [##----------------------------------]    6%  00:01:22\n",
      "validation loss record: 0.1667914241552353\n",
      "\u001b[?25l  [##----------------------------------]    8%  00:01:21\n",
      "validation loss record: 0.1570148766040802\n",
      "\u001b[?25l  [###---------------------------------]   10%  00:01:20\n",
      "validation loss record: 0.15070602297782898\n",
      "\u001b[?25l  [####--------------------------------]   12%  00:01:19\n",
      "validation loss record: 0.14635297656059265\n",
      "\u001b[?25l  [#####-------------------------------]   14%  00:01:18\n",
      "validation loss record: 0.1433272361755371\n",
      "\u001b[?25l  [#####-------------------------------]   16%  00:01:17\n",
      "validation loss record: 0.1415148377418518\n",
      "\u001b[?25l  [######------------------------------]   18%  00:01:16\n",
      "validation loss record: 0.1403128206729889\n",
      "\u001b[?25l  [#######-----------------------------]   20%  00:01:15\n",
      "validation loss record: 0.13952849805355072\n",
      "\u001b[?25l  [#######-----------------------------]   22%  00:01:13\n",
      "validation loss record: 0.13914920389652252\n",
      "\u001b[?25l  [########----------------------------]   24%  00:01:11\n",
      "validation loss record: 0.13893072307109833\n",
      "\u001b[?25l  [#########---------------------------]   26%  00:01:09\n",
      "validation loss record: 0.13880017399787903\n",
      "\u001b[?25l  [##########--------------------------]   28%  00:01:07\n",
      "validation loss record: 0.13877993822097778\n",
      "\u001b[?25l  [############------------------------]   36%  00:00:58\n",
      "validation loss record: 0.13874153792858124\n",
      "\u001b[?25l  [####################----------------]   57%  00:00:37Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "\u001b[?25l  [############################--------]   80%  00:00:17Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[?25l  [####################################]  100%          \u001b[?25h\n",
      "LOG: error plot finished and dumped to run.error.pdf\n",
      "prob_escape: Escape\n",
      "pearsonr = 0.859\n",
      "LOG: scatter plot finished and dumped to run.scatter.pdf\n",
      "WARNING: I don't know how to make a GE plot for any other dims other than 1 and 2.\n",
      "LOG: loaded data, evaluating beta coeff for wildtype seq: NITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKST\n",
      "LOG: Beta coefficients plotted and dumped to run.beta.pdf\n",
      "LOG: model loaded, calculating SVD for beta coefficents.\n",
      "LOG: Singular values of beta plotted and dumped to run.svd.pdf\n",
      "LOG: model loaded, plotting amino acid and site profiles.\n",
      "LOG: Amino acid and site profiles plotted and dumped to run.profiles.pdf\n",
      "LOG: `tdms go` completed; touching run.sentinel\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks\n"
     ]
    }
   ],
   "source": [
    "for n in [1,2,3,4]:\n",
    "    min_test_per_stratum = [200,250,300,350]\n",
    "    min_count_per_stratum = [800,1200,1600,2000]\n",
    "    \n",
    "    %cd torchdms_results/noisy_3conc_{n}muts\n",
    "    \n",
    "    !echo \"Prepping dataset.\"\n",
    "    !tdms prep --per-stratum-variants-for-test {min_test_per_stratum[n-1]} --skip-stratum-if-count-is-smaller-than {min_count_per_stratum[n-1]} \\\n",
    "    --partition-by library *data.pkl prepped prob_escape\n",
    "    \n",
    "    !echo \"Training model.\"\n",
    "    !tdms go --config config.json\n",
    "\n",
    "    %cd ../.. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
