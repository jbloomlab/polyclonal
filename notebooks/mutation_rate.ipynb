{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What mutation rate should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have simulated noisy data with average of 1, 2, 3, and 4 mutations. If we fit a model to each of them, how good are the fits? \n",
    "Each library has 30,000 variants and we will use the same parameters in each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>library</th>\n",
       "      <th>aa_substitutions</th>\n",
       "      <th>concentration</th>\n",
       "      <th>prob_escape</th>\n",
       "      <th>IC90</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td></td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.087480</td>\n",
       "      <td>0.1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td></td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.034240</td>\n",
       "      <td>0.1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td></td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.037880</td>\n",
       "      <td>0.1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td></td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.035730</td>\n",
       "      <td>0.1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td></td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359995</th>\n",
       "      <td>avg2muts</td>\n",
       "      <td>Y473E L518F D427L</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>1.1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359996</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td>Y473S G413Q</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359997</th>\n",
       "      <td>avg1muts</td>\n",
       "      <td>Y473V P479R F392W</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.160200</td>\n",
       "      <td>1.4550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359998</th>\n",
       "      <td>avg3muts</td>\n",
       "      <td>Y489Q N501Y</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359999</th>\n",
       "      <td>avg2muts</td>\n",
       "      <td>Y505N H519T</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         library   aa_substitutions  concentration  prob_escape    IC90\n",
       "0       avg1muts                              0.25     0.087480  0.1128\n",
       "1       avg1muts                              0.25     0.034240  0.1128\n",
       "2       avg1muts                              0.25     0.037880  0.1128\n",
       "3       avg1muts                              0.25     0.035730  0.1128\n",
       "4       avg1muts                              0.25     0.000000  0.1128\n",
       "...          ...                ...            ...          ...     ...\n",
       "359995  avg2muts  Y473E L518F D427L           4.00     0.002918  1.1600\n",
       "359996  avg1muts        Y473S G413Q           4.00     0.000000  0.5780\n",
       "359997  avg1muts  Y473V P479R F392W           4.00     0.160200  1.4550\n",
       "359998  avg3muts        Y489Q N501Y           4.00     0.000000  0.5881\n",
       "359999  avg2muts        Y505N H519T           4.00     0.000000  0.3505\n",
       "\n",
       "[360000 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polyclonal\n",
    "\n",
    "noisy_data = (\n",
    "    pd.read_csv('RBD_variants_escape_noisy.csv', na_filter=None)\n",
    "    .query('concentration in [0.25, 1, 4]')\n",
    "    .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "noisy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the dataset for each library into separate output directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset written to torchdms_results/noisy_3conc_1muts.\n",
      "Dataset written to torchdms_results/noisy_3conc_2muts.\n",
      "Dataset written to torchdms_results/noisy_3conc_3muts.\n",
      "Dataset written to torchdms_results/noisy_3conc_4muts.\n"
     ]
    }
   ],
   "source": [
    "import torchdms\n",
    "import Bio.SeqIO\n",
    "import pickle\n",
    "\n",
    "wtseq_dna = Bio.SeqIO.read('RBD_seq.fasta', 'fasta').seq\n",
    "wtseq_aa = str(wtseq_dna.translate())\n",
    "assert len(wtseq_aa) == 201\n",
    "\n",
    "for n in [1,2,3,4]:\n",
    "    avg_n_data = noisy_data.query(f\"library == 'avg{n}muts'\")\n",
    "    # torchdms uses 1-indexed mutations\n",
    "    formatted_data = (\n",
    "            avg_n_data\n",
    "            .assign(aa_substitutions=lambda x: x['aa_substitutions'].apply(\n",
    "                                polyclonal.utils.shift_mut_site, shift=-330)\n",
    "            )\n",
    "    )    \n",
    "    assert len(formatted_data.index) == 90000\n",
    "    with open(f\"torchdms_results/noisy_3conc_{n}muts/noisy_3conc_{n}muts_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump([formatted_data, wtseq_aa], f)\n",
    "    print(f\"Dataset written to torchdms_results/noisy_3conc_{n}muts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the `torchdms` models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks/torchdms_results/noisy_3conc_1muts\n",
      "Prepping dataset.\n",
      "LOG: Targets: ('prob_escape',)\n",
      "LOG: There are 90000 total variants in this dataset\n",
      "LOG: Partitioning data via 'avg1muts'\n",
      "LOG: There are 25587 training samples for stratum: 1\n",
      "LOG: There are 15623 training samples for stratum: 2\n",
      "LOG: There are 4423 training samples for stratum: 3\n",
      "LOG: There are 298 training samples for stratum: 4\n",
      "LOG: There are 5261 validation samples\n",
      "LOG: There are 5234 test samples\n",
      "LOG: Successfully partitioned data\n",
      "LOG: preparing binary map dataset\n",
      "LOG: Successfully finished prep and dumped SplitDataset object to prepped\n",
      "Training model.\n",
      "LOG: Setting random seed to 0.\n",
      "LOG: Model defined as: Escape(\n",
      "  (latent_layer_epi0): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi1): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi2): Linear(in_features=4221, out_features=1, bias=False)\n",
      ")\n",
      "{'num_epitopes': 3, 'beta_l1_coefficient': 0.0}\n",
      "LOG: Saved model to run.model\n",
      "LOG: Setting random seed to 0.\n",
      "Exponentiating targets with base 1.\n",
      "Starting training. {'independent_start_count': 0, 'independent_start_epoch_count': None, 'epoch_count': 50, 'loss_fn': <function weighted_loss.<locals>.wrapper at 0x14cb772998b0>, 'patience': 10, 'min_lr': 1e-05, 'loss_weight_span': None, 'exp_target': 1, 'beta_rank': None}\n",
      "LOG: Beginning full training.\n",
      "Training in default style.\n",
      "\n",
      "validation loss record: 0.10647932440042496\n",
      "\u001b[?25l  [------------------------------------]    2%  00:01:14\n",
      "validation loss record: 0.10238621383905411\n",
      "\u001b[?25l  [#-----------------------------------]    4%  00:01:12\n",
      "validation loss record: 0.0986446812748909\n",
      "\u001b[?25l  [##----------------------------------]    6%  00:01:12\n",
      "validation loss record: 0.09571362286806107\n",
      "\u001b[?25l  [##----------------------------------]    8%  00:01:11\n",
      "validation loss record: 0.09362868219614029\n",
      "\u001b[?25l  [###---------------------------------]   10%  00:01:11\n",
      "validation loss record: 0.09206327050924301\n",
      "\u001b[?25l  [####--------------------------------]   12%  00:01:12\n",
      "validation loss record: 0.09085451066493988\n",
      "\u001b[?25l  [#####-------------------------------]   14%  00:01:14\n",
      "validation loss record: 0.08989318460226059\n",
      "\u001b[?25l  [#####-------------------------------]   16%  00:01:15\n",
      "validation loss record: 0.08916480094194412\n",
      "\u001b[?25l  [######------------------------------]   18%  00:01:17\n",
      "validation loss record: 0.08868575841188431\n",
      "\u001b[?25l  [#######-----------------------------]   20%  00:01:17\n",
      "validation loss record: 0.08831045031547546\n",
      "\u001b[?25l  [#######-----------------------------]   22%  00:01:17\n",
      "validation loss record: 0.08801887184381485\n",
      "\u001b[?25l  [########----------------------------]   24%  00:01:16\n",
      "validation loss record: 0.08776845037937164\n",
      "\u001b[?25l  [#########---------------------------]   26%  00:01:14\n",
      "validation loss record: 0.08764990419149399\n",
      "\u001b[?25l  [##########--------------------------]   30%  00:01:07\n",
      "validation loss record: 0.08756635338068008\n",
      "\u001b[?25l  [###########-------------------------]   32%  00:01:04\n",
      "validation loss record: 0.08748286962509155\n",
      "\u001b[?25l  [############------------------------]   34%  00:01:01\n",
      "validation loss record: 0.08737649023532867\n",
      "\u001b[?25l  [############------------------------]   36%  00:00:58\n",
      "validation loss record: 0.08734744042158127\n",
      "\u001b[?25l  [#############-----------------------]   38%  00:00:56\n",
      "validation loss record: 0.0872141495347023\n",
      "\u001b[?25l  [##############----------------------]   40%  00:00:53\n",
      "validation loss record: 0.08715954422950745\n",
      "\u001b[?25l  [###############---------------------]   42%  00:00:51\n",
      "validation loss record: 0.08700349181890488\n",
      "\u001b[?25l  [###############---------------------]   44%  00:00:49\n",
      "validation loss record: 0.0869278684258461\n",
      "\u001b[?25l  [################--------------------]   46%  00:00:47\n",
      "validation loss record: 0.08680714666843414\n",
      "\u001b[?25l  [#################-------------------]   48%  00:00:45\n",
      "validation loss record: 0.0867210179567337\n",
      "\u001b[?25l  [##################------------------]   50%  00:00:43\n",
      "validation loss record: 0.08669278025627136\n",
      "\u001b[?25l  [##################------------------]   52%  00:00:41\n",
      "validation loss record: 0.08666210621595383\n",
      "\u001b[?25l  [###################-----------------]   54%  00:00:39\n",
      "validation loss record: 0.08660206198692322\n",
      "\u001b[?25l  [####################----------------]   56%  00:00:37\n",
      "validation loss record: 0.08659114688634872\n",
      "\u001b[?25l  [####################----------------]   57%  00:00:35\n",
      "validation loss record: 0.08658815920352936\n",
      "\u001b[?25l  [#####################---------------]   60%  00:00:33\n",
      "validation loss record: 0.08653493225574493\n",
      "\u001b[?25l  [######################--------------]   62%  00:00:32\n",
      "validation loss record: 0.0865146741271019\n",
      "\u001b[?25l  [##############################------]   84%  00:00:13Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "\u001b[?25l  [####################################]  100%          \u001b[?25h\n",
      "LOG: error plot finished and dumped to run.error.pdf\n",
      "prob_escape: Escape\n",
      "pearsonr = 0.68\n",
      "LOG: scatter plot finished and dumped to run.scatter.pdf\n",
      "WARNING: I don't know how to make a GE plot for any other dims other than 1 and 2.\n",
      "LOG: loaded data, evaluating beta coeff for wildtype seq: NITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKST\n",
      "LOG: Beta coefficients plotted and dumped to run.beta.pdf\n",
      "LOG: model loaded, calculating SVD for beta coefficents.\n",
      "LOG: Singular values of beta plotted and dumped to run.svd.pdf\n",
      "LOG: model loaded, plotting amino acid and site profiles.\n",
      "LOG: Amino acid and site profiles plotted and dumped to run.profiles.pdf\n",
      "LOG: `tdms go` completed; touching run.sentinel\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks/torchdms_results/noisy_3conc_2muts\n",
      "Prepping dataset.\n",
      "LOG: Targets: ('prob_escape',)\n",
      "LOG: There are 90000 total variants in this dataset\n",
      "LOG: Partitioning data via 'avg2muts'\n",
      "LOG: There are 17907 training samples for stratum: 1\n",
      "LOG: There are 22493 training samples for stratum: 2\n",
      "LOG: There are 15017 training samples for stratum: 3\n",
      "LOG: There are 6850 training samples for stratum: 4\n",
      "LOG: There are 1843 training samples for stratum: 5\n",
      "LOG: There are 6108 validation samples\n",
      "LOG: There are 5976 test samples\n",
      "LOG: Successfully partitioned data\n",
      "LOG: preparing binary map dataset\n",
      "LOG: Successfully finished prep and dumped SplitDataset object to prepped\n",
      "Training model.\n",
      "LOG: Setting random seed to 0.\n",
      "LOG: Model defined as: Escape(\n",
      "  (latent_layer_epi0): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi1): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi2): Linear(in_features=4221, out_features=1, bias=False)\n",
      ")\n",
      "{'num_epitopes': 3, 'beta_l1_coefficient': 0.0}\n",
      "LOG: Saved model to run.model\n",
      "LOG: Setting random seed to 0.\n",
      "Exponentiating targets with base 1.\n",
      "Starting training. {'independent_start_count': 0, 'independent_start_epoch_count': None, 'epoch_count': 50, 'loss_fn': <function weighted_loss.<locals>.wrapper at 0x149351abb8b0>, 'patience': 10, 'min_lr': 1e-05, 'loss_weight_span': None, 'exp_target': 1, 'beta_rank': None}\n",
      "LOG: Beginning full training.\n",
      "Training in default style.\n",
      "\n",
      "validation loss record: 0.14212356507778168\n",
      "\u001b[?25l  [------------------------------------]    2%  00:01:20\n",
      "validation loss record: 0.12766872346401215\n",
      "\u001b[?25l  [#-----------------------------------]    4%  00:01:20\n",
      "validation loss record: 0.12044548988342285\n",
      "\u001b[?25l  [##----------------------------------]    6%  00:01:19\n",
      "validation loss record: 0.11662011593580246\n",
      "\u001b[?25l  [##----------------------------------]    8%  00:01:18\n",
      "validation loss record: 0.11422984302043915\n",
      "\u001b[?25l  [###---------------------------------]   10%  00:01:16\n",
      "validation loss record: 0.11280222982168198\n",
      "\u001b[?25l  [####--------------------------------]   12%  00:01:15\n",
      "validation loss record: 0.11187304556369781\n",
      "\u001b[?25l  [#####-------------------------------]   14%  00:01:13\n",
      "validation loss record: 0.11114194244146347\n",
      "\u001b[?25l  [#####-------------------------------]   16%  00:01:11\n",
      "validation loss record: 0.11103921383619308\n",
      "\u001b[?25l  [######------------------------------]   18%  00:01:09\n",
      "validation loss record: 0.11073428392410278\n",
      "\u001b[?25l  [#######-----------------------------]   22%  00:01:05\n",
      "validation loss record: 0.11053729802370071\n",
      "\u001b[?25l  [########----------------------------]   24%  00:01:03\n",
      "validation loss record: 0.11053580045700073\n",
      "\u001b[?25l  [#########---------------------------]   26%  00:01:01\n",
      "validation loss record: 0.11043830960988998\n",
      "\u001b[?25l  [##########--------------------------]   28%  00:00:59\n",
      "validation loss record: 0.11029478162527084\n",
      "\u001b[?25l  [##########--------------------------]   30%  00:00:57\n",
      "validation loss record: 0.11025568097829819\n",
      "\u001b[?25l  [############------------------------]   34%  00:00:54\n",
      "validation loss record: 0.11022091656923294\n",
      "\u001b[?25l  [############------------------------]   36%  00:00:52\n",
      "validation loss record: 0.11010169237852097\n",
      "\u001b[?25l  [#############-----------------------]   38%  00:00:50\n",
      "validation loss record: 0.1099979430437088\n",
      "\u001b[?25l  [###############---------------------]   42%  00:00:47\n",
      "validation loss record: 0.10987783968448639\n",
      "\u001b[?25l  [#######################-------------]   64%  00:00:28Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "\u001b[?25l  [##############################------]   86%  00:00:11Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[?25l  [####################################]  100%          \u001b[?25h\n",
      "LOG: error plot finished and dumped to run.error.pdf\n",
      "prob_escape: Escape\n",
      "pearsonr = 0.754\n",
      "LOG: scatter plot finished and dumped to run.scatter.pdf\n",
      "WARNING: I don't know how to make a GE plot for any other dims other than 1 and 2.\n",
      "LOG: loaded data, evaluating beta coeff for wildtype seq: NITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKST\n",
      "LOG: Beta coefficients plotted and dumped to run.beta.pdf\n",
      "LOG: model loaded, calculating SVD for beta coefficents.\n",
      "LOG: Singular values of beta plotted and dumped to run.svd.pdf\n",
      "LOG: model loaded, plotting amino acid and site profiles.\n",
      "LOG: Amino acid and site profiles plotted and dumped to run.profiles.pdf\n",
      "LOG: `tdms go` completed; touching run.sentinel\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks/torchdms_results/noisy_3conc_3muts\n",
      "Prepping dataset.\n",
      "LOG: Targets: ('prob_escape',)\n",
      "LOG: There are 90000 total variants in this dataset\n",
      "LOG: Partitioning data via 'avg3muts'\n",
      "LOG: There are 8695 training samples for stratum: 1\n",
      "LOG: There are 18431 training samples for stratum: 2\n",
      "LOG: There are 17957 training samples for stratum: 3\n",
      "LOG: There are 13635 training samples for stratum: 4\n",
      "LOG: There are 7555 training samples for stratum: 5\n",
      "LOG: There are 2863 training samples for stratum: 6\n",
      "LOG: There are 274 training samples for stratum: 7\n",
      "LOG: There are 7454 validation samples\n",
      "LOG: There are 7328 test samples\n",
      "LOG: Successfully partitioned data\n",
      "LOG: preparing binary map dataset\n",
      "LOG: Successfully finished prep and dumped SplitDataset object to prepped\n",
      "Training model.\n",
      "LOG: Setting random seed to 0.\n",
      "LOG: Model defined as: Escape(\n",
      "  (latent_layer_epi0): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi1): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi2): Linear(in_features=4221, out_features=1, bias=False)\n",
      ")\n",
      "{'num_epitopes': 3, 'beta_l1_coefficient': 0.0}\n",
      "LOG: Saved model to run.model\n",
      "LOG: Setting random seed to 0.\n",
      "Exponentiating targets with base 1.\n",
      "Starting training. {'independent_start_count': 0, 'independent_start_epoch_count': None, 'epoch_count': 50, 'loss_fn': <function weighted_loss.<locals>.wrapper at 0x14ece1cba8b0>, 'patience': 10, 'min_lr': 1e-05, 'loss_weight_span': None, 'exp_target': 1, 'beta_rank': None}\n",
      "LOG: Beginning full training.\n",
      "Training in default style.\n",
      "\n",
      "validation loss record: 0.21770977973937988\n",
      "\u001b[?25l  [------------------------------------]    2%  00:01:28\n",
      "validation loss record: 0.19148309528827667\n",
      "\u001b[?25l  [#-----------------------------------]    4%  00:01:26\n",
      "validation loss record: 0.17264260351657867\n",
      "\u001b[?25l  [##----------------------------------]    6%  00:01:26\n",
      "validation loss record: 0.15950913727283478\n",
      "\u001b[?25l  [##----------------------------------]    8%  00:01:25\n",
      "validation loss record: 0.15096436440944672\n",
      "\u001b[?25l  [###---------------------------------]   10%  00:01:24\n",
      "validation loss record: 0.14539258182048798\n",
      "\u001b[?25l  [####--------------------------------]   12%  00:01:22\n",
      "validation loss record: 0.14191202819347382\n",
      "\u001b[?25l  [#####-------------------------------]   14%  00:01:21\n",
      "validation loss record: 0.1397085189819336\n",
      "\u001b[?25l  [#####-------------------------------]   16%  00:01:19\n",
      "validation loss record: 0.13831129670143127\n",
      "\u001b[?25l  [######------------------------------]   18%  00:01:18\n",
      "validation loss record: 0.13769493997097015\n",
      "\u001b[?25l  [#######-----------------------------]   20%  00:01:16\n",
      "validation loss record: 0.1374277025461197\n",
      "\u001b[?25l  [#######-----------------------------]   22%  00:01:14\n",
      "validation loss record: 0.1372392177581787\n",
      "\u001b[?25l  [##########--------------------------]   28%  00:01:09\n",
      "validation loss record: 0.13721904158592224\n",
      "\u001b[?25l  [############------------------------]   36%  00:01:03\n",
      "validation loss record: 0.13716839253902435\n",
      "\u001b[?25l  [###############---------------------]   42%  00:00:56\n",
      "validation loss record: 0.137137308716774\n",
      "\u001b[?25l  [#######################-------------]   64%  00:00:35Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "\u001b[?25l  [##############################------]   86%  00:00:15Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[?25l  [####################################]  100%          \u001b[?25h\n",
      "LOG: error plot finished and dumped to run.error.pdf\n",
      "prob_escape: Escape\n",
      "pearsonr = 0.837\n",
      "LOG: scatter plot finished and dumped to run.scatter.pdf\n",
      "WARNING: I don't know how to make a GE plot for any other dims other than 1 and 2.\n",
      "LOG: loaded data, evaluating beta coeff for wildtype seq: NITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKST\n",
      "LOG: Beta coefficients plotted and dumped to run.beta.pdf\n",
      "LOG: model loaded, calculating SVD for beta coefficents.\n",
      "LOG: Singular values of beta plotted and dumped to run.svd.pdf\n",
      "LOG: model loaded, plotting amino acid and site profiles.\n",
      "LOG: Amino acid and site profiles plotted and dumped to run.profiles.pdf\n",
      "LOG: `tdms go` completed; touching run.sentinel\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks/torchdms_results/noisy_3conc_4muts\n",
      "Prepping dataset.\n",
      "LOG: Targets: ('prob_escape',)\n",
      "LOG: There are 90000 total variants in this dataset\n",
      "LOG: Partitioning data via 'avg4muts'\n",
      "LOG: There are 3135 training samples for stratum: 1\n",
      "LOG: There are 11099 training samples for stratum: 2\n",
      "LOG: There are 15517 training samples for stratum: 3\n",
      "LOG: There are 15118 training samples for stratum: 4\n",
      "LOG: There are 12299 training samples for stratum: 5\n",
      "LOG: There are 7507 training samples for stratum: 6\n",
      "LOG: There are 3512 training samples for stratum: 7\n",
      "LOG: There are 623 training samples for stratum: 8\n",
      "LOG: There are 8870 validation samples\n",
      "LOG: There are 8677 test samples\n",
      "LOG: Successfully partitioned data\n",
      "LOG: preparing binary map dataset\n",
      "LOG: Successfully finished prep and dumped SplitDataset object to prepped\n",
      "Training model.\n",
      "LOG: Setting random seed to 0.\n",
      "LOG: Model defined as: Escape(\n",
      "  (latent_layer_epi0): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi1): Linear(in_features=4221, out_features=1, bias=False)\n",
      "  (latent_layer_epi2): Linear(in_features=4221, out_features=1, bias=False)\n",
      ")\n",
      "{'num_epitopes': 3, 'beta_l1_coefficient': 0.0}\n",
      "LOG: Saved model to run.model\n",
      "LOG: Setting random seed to 0.\n",
      "Exponentiating targets with base 1.\n",
      "Starting training. {'independent_start_count': 0, 'independent_start_epoch_count': None, 'epoch_count': 50, 'loss_fn': <function weighted_loss.<locals>.wrapper at 0x14eac31928b0>, 'patience': 10, 'min_lr': 1e-05, 'loss_weight_span': None, 'exp_target': 1, 'beta_rank': None}\n",
      "LOG: Beginning full training.\n",
      "Training in default style.\n",
      "\n",
      "validation loss record: 0.2207835614681244\n",
      "\u001b[?25l  [------------------------------------]    2%  00:01:24\n",
      "validation loss record: 0.20026332139968872\n",
      "\u001b[?25l  [#-----------------------------------]    4%  00:01:22\n",
      "validation loss record: 0.18182939291000366\n",
      "\u001b[?25l  [##----------------------------------]    6%  00:01:20\n",
      "validation loss record: 0.16798187792301178\n",
      "\u001b[?25l  [##----------------------------------]    8%  00:01:19\n",
      "validation loss record: 0.1582634598016739\n",
      "\u001b[?25l  [###---------------------------------]   10%  00:01:18\n",
      "validation loss record: 0.15197934210300446\n",
      "\u001b[?25l  [####--------------------------------]   12%  00:01:16\n",
      "validation loss record: 0.14778855443000793\n",
      "\u001b[?25l  [#####-------------------------------]   14%  00:01:15\n",
      "validation loss record: 0.14532969892024994\n",
      "\u001b[?25l  [#####-------------------------------]   16%  00:01:14\n",
      "validation loss record: 0.1434444636106491\n",
      "\u001b[?25l  [######------------------------------]   18%  00:01:12\n",
      "validation loss record: 0.1422722339630127\n",
      "\u001b[?25l  [#######-----------------------------]   20%  00:01:11\n",
      "validation loss record: 0.14132842421531677\n",
      "\u001b[?25l  [#######-----------------------------]   22%  00:01:10\n",
      "validation loss record: 0.1410713791847229\n",
      "\u001b[?25l  [########----------------------------]   24%  00:01:08\n",
      "validation loss record: 0.140677809715271\n",
      "\u001b[?25l  [#########---------------------------]   26%  00:01:06\n",
      "validation loss record: 0.14045360684394836\n",
      "\u001b[?25l  [##########--------------------------]   28%  00:01:04\n",
      "validation loss record: 0.14043676853179932\n",
      "\u001b[?25l  [###########-------------------------]   32%  00:01:00\n",
      "validation loss record: 0.14041121304035187\n",
      "\u001b[?25l  [#############-----------------------]   38%  00:00:55\n",
      "validation loss record: 0.14040949940681458\n",
      "\u001b[?25l  [###############---------------------]   42%  00:00:51\n",
      "validation loss record: 0.14032785594463348\n",
      "\u001b[?25l  [###############---------------------]   44%  00:00:49\n",
      "validation loss record: 0.14030945301055908\n",
      "\u001b[?25l  [################--------------------]   46%  00:00:48\n",
      "validation loss record: 0.14022453129291534\n",
      "\u001b[?25l  [##################------------------]   52%  00:00:42\n",
      "validation loss record: 0.1401279866695404\n",
      "\u001b[?25l  [##########################----------]   74%  00:00:23Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "\u001b[?25l  [##################################--]   96%  00:00:03Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\u001b[?25l  [####################################]  100%          \u001b[?25h\n",
      "LOG: error plot finished and dumped to run.error.pdf\n",
      "prob_escape: Escape\n",
      "pearsonr = 0.857\n",
      "LOG: scatter plot finished and dumped to run.scatter.pdf\n",
      "WARNING: I don't know how to make a GE plot for any other dims other than 1 and 2.\n",
      "LOG: loaded data, evaluating beta coeff for wildtype seq: NITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKST\n",
      "LOG: Beta coefficients plotted and dumped to run.beta.pdf\n",
      "LOG: model loaded, calculating SVD for beta coefficents.\n",
      "LOG: Singular values of beta plotted and dumped to run.svd.pdf\n",
      "LOG: model loaded, plotting amino acid and site profiles.\n",
      "LOG: Amino acid and site profiles plotted and dumped to run.profiles.pdf\n",
      "LOG: `tdms go` completed; touching run.sentinel\n",
      "/fh/fast/bloom_j/computational_notebooks/tyu2/2021/polyclonal/notebooks\n"
     ]
    }
   ],
   "source": [
    "for n in [1,2,3,4]:\n",
    "    min_test_per_stratum = [200,250,300,350]\n",
    "    min_count_per_stratum = [800,1200,1600,2000]\n",
    "    \n",
    "    %cd torchdms_results/noisy_3conc_{n}muts\n",
    "    \n",
    "    !echo \"Prepping dataset.\"\n",
    "    !tdms prep --per-stratum-variants-for-test {min_test_per_stratum[n-1]} --skip-stratum-if-count-is-smaller-than {min_count_per_stratum[n-1]} \\\n",
    "    --partition-by library *data.pkl prepped prob_escape\n",
    "    \n",
    "    !echo \"Training model.\"\n",
    "    !tdms go --config config.json\n",
    "\n",
    "    %cd ../.. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
